{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "\n",
    "For this problem set, you will expand on PS2 to perform and evaluate various sentiment classification methods.\n",
    "\n",
    "Your name: Raegan Gutierrez\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (1 point)\n",
    "\n",
    "For this step, you will load the training and test sentiment datasets \"twitdata_TEST.tsv\" and \"allTrainingData.tsv\". The data should be loaded into 4 lists of strings: X_txt_train, X_txt_test, y_test, y_train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "X_txt_train = []\n",
    "y_train = []\n",
    "X_txt_test = []\n",
    "y_test = []\n",
    "\n",
    "# Loading data from CSVs.\n",
    "\n",
    "train= open('allTrainingData.tsv')\n",
    "traincsv = csv.reader(train, delimiter = '\\t', quoting = csv.QUOTE_NONE)\n",
    "\n",
    "test= open('twitdata_TEST.tsv')\n",
    "testcsv = csv.reader(test, delimiter = '\\t', quoting = csv.QUOTE_NONE)\n",
    "    \n",
    "\n",
    "# 1. Load the training datasets into two lists (X_txt_train will be a list of strings; y_train)\n",
    "\n",
    "for row in traincsv:\n",
    "    y_train.append(row[2])\n",
    "    X_txt_train.append(row[3])\n",
    "\n",
    "    \n",
    "# 2. Load the test datasets into two lists (X_txt_test will be a list of strings; y_test)\n",
    "\n",
    "for row in testcsv:\n",
    "    y_test.append(row[2])\n",
    "    X_txt_test.append(row[3])\n",
    "    \n",
    "train.close()\n",
    "test.close()\n",
    "\n",
    "type(X_txt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(X_txt_train) == type(list()))\n",
    "assert(type(X_txt_train[0]) == type(str()))\n",
    "assert(type(X_txt_test) == type(list()))\n",
    "assert(type(X_txt_test[0]) == type(str()))\n",
    "assert(type(y_test) == type(list()))\n",
    "assert(type(y_train) == type(list()))\n",
    "assert(len(X_txt_test) == 3199)\n",
    "assert(len(y_test) == 3199)\n",
    "assert(len(X_txt_train) == 8018)\n",
    "assert(len(y_train) == 8018)\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (2 point)\n",
    "\n",
    "This part is similar to HW2 (using the positive_words and negative_words variables). We will compare last homework's lexicon-based classification method with supervised models. Only make predictions on the test split and store all predictions in the list lex_test_preds. Next, calculate the **macro** precision, macro recall, and macro f1 scores using the lex_test_preds list.\n",
    "\n",
    "You can learn more about lexicon-based classification in Chapter 19.6. If you are interested, the chapter is available online for free at the following link: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/19.pdf)\n",
    "\n",
    "\n",
    "**INTUITION:** For PS2 you implemented a \"lexicon-based classifier\". You looked at a few examples and manually accessed it's performance. Howeover, that was arbitary. Now we want to see how well it actually works. Hence, in this homework (for this exercise), you will use the class I provided that implements the lexicon-based classifier and use the provided \"annotatoed dataset\" (loaded in Exercise 1) to see how well it performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LexiconClassifier():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Initalize the Lexicon classifer by loading lexicons. \n",
    "        \"\"\"\n",
    "        self.positive_words = set()\n",
    "        with open('positive-words.txt', encoding = 'utf-8') as iFile:\n",
    "            for row in iFile:\n",
    "                self.positive_words.add(row.strip())\n",
    "\n",
    "        self.negative_words = set()\n",
    "        with open('negative-words.txt', encoding='iso-8859-1') as iFile:\n",
    "            for row in iFile:\n",
    "                self.negative_words.add(row.strip())\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        \"\"\"\n",
    "            Returns a sentiment prediction give an input string.\n",
    "            \n",
    "            Keyword arguments:\n",
    "            sentence -- string (e.g., \"This is good good good\")\n",
    "            \n",
    "            Returns:\n",
    "            pred -- a string (\"postive, \"negative\", or \"neutral\")\n",
    "        \"\"\"\n",
    "        num_pos_words = 0\n",
    "        num_neg_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.positive_words:\n",
    "                num_pos_words += 1\n",
    "            elif word in self.negative_words:\n",
    "                num_neg_words += 1\n",
    "        \n",
    "        pred = 'neutral'        \n",
    "        if num_pos_words > num_neg_words:\n",
    "            pred = 'positive'\n",
    "        elif num_pos_words < num_neg_words:\n",
    "            pred = 'negative'\n",
    "            \n",
    "        return pred\n",
    "    \n",
    "    def count_pos_words(self, sentence):\n",
    "        \"\"\"\n",
    "            Returns the number of positive words in string\n",
    "            \n",
    "            Keyword arguments:\n",
    "            sentence -- string (e.g., \"This is good good good\")\n",
    "            \n",
    "            Returns:\n",
    "            pred -- an integer (e.g., 3)\n",
    "        \"\"\"\n",
    "        num_pos_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.positive_words:\n",
    "                num_pos_words += 1\n",
    "        return num_pos_words\n",
    "\n",
    "    def count_neg_words(self, sentence):\n",
    "        \"\"\"\n",
    "            Returns the number of negative words in string\n",
    "            \n",
    "            Keyword arguments:\n",
    "            sentence -- string (e.g., \"This is good good good\")\n",
    "            \n",
    "            Returns:\n",
    "            pred -- an integer (e.g., 3)\n",
    "        \"\"\"\n",
    "        num_neg_words = 0\n",
    "        for word in sentence.lower().split():\n",
    "            if word in self.negative_words:\n",
    "                num_neg_words += 1\n",
    "        return num_neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5504\n",
      "Recall: 0.5446\n",
      "F1: 0.5455\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "lex_test_preds = [] \n",
    "\n",
    "Lexi = LexiconClassifier()\n",
    "for string in X_txt_test:\n",
    "    pred = Lexi.predict(string)\n",
    "    lex_test_preds.append(pred)\n",
    "\n",
    "precision = precision_score(y_test, lex_test_preds, average = \"macro\")\n",
    "recall = recall_score(y_test, lex_test_preds, average = \"macro\")\n",
    "f1 = f1_score(y_test, lex_test_preds, average = \"macro\")\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_txt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(lex_test_preds) == type(list()))\n",
    "assert(type(lex_test_preds[0]) == type(str()))\n",
    "assert(set(lex_test_preds) == set([\"positive\", \"negative\", \"neutral\"]))\n",
    "assert(len(lex_test_preds) == len(y_test))\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (1 point)\n",
    "\n",
    "Again, using the LexiconClassifier, write code to generate a lists of lists where each sublist contains the number of positive words and negative words in a tweet. For example, assume we are given the following train and test datasets\n",
    "\n",
    "``` python\n",
    "X_txt_train = [\"good good\", \"bad bad\"]\n",
    "X_txt_test = [\"great\", \"bad bad great\"]\n",
    "```\n",
    "\n",
    "you should write code that creates two lists of lists as follows:\n",
    "\n",
    "``` python\n",
    "X_train_lexicon_features = [[2, 0], [0,2]]\n",
    "X_test_lexicon_features = [[1, 0], [1, 2]]\n",
    "```\n",
    "\n",
    "Why are we doing this? We will use these as addition features in Exercise 5, combining it with the ngram features. Combining different sets of features is called \"Feature Engineering\" and is one of the most important steps of many machine learning tasks. In this case, we are using the lexicons to generate additional features. But, we could also count the number of capitalized words, number of punctuation marks, etc. We would come up with different feature sets via trial-and-error. We can guess what type of features would help our task. For instance, for sentiment prediction, we may guess that having many capitalized words is predictive of something negative (e.g., \"WHY ARE YOU DOING THIS!!!!!\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_lexicon_features = [] \n",
    "X_test_lexicon_features = [] \n",
    "\n",
    "\n",
    "for string in X_txt_test:\n",
    "    pos = Lexi.count_pos_words(string)\n",
    "    neg = Lexi.count_neg_words(string)\n",
    "    X_test_lexicon_features.append([pos,neg])\n",
    "\n",
    "\n",
    "for string in X_txt_train:\n",
    "    pos = Lexi.count_pos_words(string)\n",
    "    neg = Lexi.count_neg_words(string)\n",
    "    X_train_lexicon_features.append([pos,neg])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "assert(type(X_train_lexicon_features) == type(list()))\n",
    "assert(type(X_test_lexicon_features) == type(list()))\n",
    "assert(type(X_test_lexicon_features[0]) == type(list()))\n",
    "assert(len(X_train_lexicon_features) == len(X_txt_train))\n",
    "assert(len(X_test_lexicon_features) == len(X_txt_test))\n",
    "assert(len(X_train_lexicon_features[0]) == 2)\n",
    "assert(len(X_test_lexicon_features[0]) == 2)\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (2 points)\n",
    "\n",
    "For this task you should creat a feature matrix using CountVectorizer and train a LinearSVC model from scikit-learn. On the train split, use GridSearchCV to find the best LinearSVC C values (0.0001, 0.001, 0.001, 0.01, 0.1, 1, 10, or 100) based on the **macro** f1 scoring metric (hint: \"macro\" average) and set the cv parameter to 5. Also, with the CountVectorizer, only use unigrams (i.e., set ngram_range = (1,1)). Note that GridSearchCV will retrain the final classifier using the best parameters, so you don't need to do it manually.\n",
    "\n",
    "**INTUITION:** For this exercise, you are implementing a simple linear model using bag-of-words features. This is generally a very strong and simple baseline for text classification. Compare the scores from this exercise to the results in Exercise 2. You will find that the machine learning-based model implemented here acheives better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.5921\n",
      "Precision: 0.6525\n",
      "Recall: 0.5740\n",
      "F1: 0.5878\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "vec = CountVectorizer(ngram_range = (1,1))\n",
    "X_train = vec.fit_transform(X_txt_train) \n",
    "X_test = vec.transform(X_txt_test) \n",
    "\n",
    " \n",
    "svc = LinearSVC()\n",
    "\n",
    "params = {'C': [0.0001, 0.001, 0.01, 0.1, 1., 10., 100.]}\n",
    "\n",
    "clf = GridSearchCV(svc, params, scoring = 'f1_macro', cv=5)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "validation_score = clf.best_score_ \n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "svm_test_predictions = clf.predict(X_test) \n",
    "\n",
    "precision = precision_score(y_test, svm_test_predictions, average = \"macro\")\n",
    "recall = recall_score(y_test, svm_test_predictions, average = \"macro\")\n",
    "f1 = f1_score(y_test, svm_test_predictions, average = \"macro\")\n",
    "\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "assert(type(X_train) == type(csr_matrix(0)) or type(X_train) == type(np.array(0)))\n",
    "assert(type(X_test) == type(csr_matrix(0)) or type(X_test) == type(np.array(0)))\n",
    "assert(X_train.shape[0] == len(X_txt_train))\n",
    "assert(X_test.shape[0] == len(X_txt_test))\n",
    "assert(X_train.shape[1] == X_test.shape[1])\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (2 points)\n",
    "\n",
    "Repeat the experiment from exercise 4, but include the lexicon features (from exercise 3) with the CountVectorizer features. Specifically, you need to concatenate the variables ```X_train_lexicon_features``` and ```X_test_lexicon_features``` with ```X_train``` and ```X_test```, respectively. Intuitively, we are performing feature engineering by adding \"lexicon features\".\n",
    "\n",
    "**INTUITION:** How do we improve the model Exercise 4? You could try different machine learning models. However, it is generally better to focus on feature engineering. What information can you provide the model to make better predictions? Here we will try to combine counts using the Lexicon from Exercise 3 as additional features (i.e., combined with the bag-of-word features) from Exercise 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.6127\n",
      "Precision: 0.6617\n",
      "Recall: 0.5977\n",
      "F1: 0.6124\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "X_train_w_lex = vec.fit_transform(X_txt_train)\n",
    "X_test_w_lex = vec.transform(X_txt_test)\n",
    "\n",
    "\n",
    "X_train_lexicon_features = np.array(X_train_lexicon_features)\n",
    "X_test_lexicon_features = np.array(X_test_lexicon_features)\n",
    "\n",
    "\n",
    "X_train_w_lex = hstack([X_train_w_lex, X_train_lexicon_features])\n",
    "X_test_w_lex = hstack([X_test_w_lex, X_test_lexicon_features])\n",
    "\n",
    "\n",
    "svc = LinearSVC()\n",
    "\n",
    "params = {'C': [0.0001, 0.001, 0.01, 0.1, 1., 10., 100.]}\n",
    "\n",
    "clf = GridSearchCV(svc, params, scoring = 'f1_macro', cv=5)\n",
    "\n",
    "clf.fit(X_train_w_lex, y_train)\n",
    "\n",
    "validation_score = clf.best_score_\n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "svm_lex_test_predictions = clf.predict(X_test_w_lex) \n",
    "\n",
    "precision = precision_score(y_test, svm_lex_test_predictions, average = \"macro\")\n",
    "recall = recall_score(y_test, svm_lex_test_predictions, average = \"macro\")\n",
    "f1 = f1_score(y_test, svm_lex_test_predictions, average = \"macro\")\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines below give example inputs and correct outputs using asserts, and can be run to test the code. Passing these tests is necessary, but **NOT** sufficient to guarantee your implementation is correct. You may add additional test cases, but do not remove any tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asserts Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "assert(X_train_w_lex.shape[0] == len(X_txt_train))\n",
    "assert(X_test.shape[0] == len(X_txt_test))\n",
    "assert(X_train_w_lex.shape[1] == X_test.shape[1] + 2)\n",
    "assert(X_train_w_lex.shape[1] == X_test_w_lex.shape[1])\n",
    "assert(type(precision) == type(float()) or type(precision) == type(np.float64()))\n",
    "assert(type(recall) == type(float()) or type(recall) == type(np.float64()))\n",
    "assert(type(f1) == type(float()) or type(f1) == type(np.float64()))\n",
    "print(\"Asserts Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (1 point)\n",
    "\n",
    "For this exercise, you will perform manual analysis of the predictions. Answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Musical awareness: Great Big Beautiful Tomorrow has an ending, Now is the time does not\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: On Radio786 100.4fm 7:10 Fri Oct 19 Labour analyst Shawn Hattingh: Cosatu's role in the context of unrest in the mining http://t.co/46pjzzl6\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: Kapan sih lo ngebuktiin,jan ngomong doang Susah Susah.usaha Aja blm udh nyerah,inget.if you never try you'll never know.cowok kok gentle bgt\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: Tomorrow come and hear @DavidWillettsMP&amp;@MASieghart debate \"Navigating the new Higher Education market\" 5.30pm, Jurys Inn #CPC12\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: Excuse the connectivity of this live stream, from Baba Amr, so many activists using only one Sat Modem. LIVE http://t.co/U283IhZ5 #Homs\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: Show your LOVE for your local field &amp; it might win an award!  Gallagher Park #Bedlington current 4th in National Award http://t.co/WeiMDtQt\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: @firecore Can you tell me when an update for the Apple TV 3rd gen becomes available? The missing update holds me back from buying #appletv3\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: @Heavensbasement The Crown, Filthy McNastys, Katy Dalys or the Duke if York in Belfast! Can't wait to catch you guys tomorrow night!\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: Uncover the Eternal City! Return flights to Rome travel on the 21st January, for 3 nights Augustea, 3 star Hotel... http://t.co/tw0Jeh9g\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: My #cre blog Oklahoma Per Square Foot returns to the @JournalRecord blog hub tomorrow. I will have some interesting local data to share.\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: \"@bbcburnsy: Loads from SB; talks with Chester continue; no deals 4 out of contract players 'til Jan; Dev t Roth ,Coops to Chest'ld #hcafc\"\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: negative\n",
      "SVM+Lexicon Prediction: negative\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: Trey Burke has been suspended for the Northern Michigan game (exhibition) tomorrow. http://t.co/oefkAElW\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: W.O.W Wednesday!Marni lands this Lumberjack vest for the ladies looking to bring a little Tom boy toughness  http://t.co/7NyCbdJR\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: Activists in Deir Ezzor captured this image of Musab Bin Umair Mosque after regime forces set it on fire Wednesday. http://t.co/MRcoprCE\n",
      "Ground-Truth Class: negative\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: @karaotr You will appreciate this.. Sunday brunch coffee: Normal cup in b/g and then the BOWL of java. Yowza. http://t.co/XhbtaCvm\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: Join me Wed for a live webcast on cost optimization for IT, for the SMB crowd. http://t.co/tyJn4RES  &lt;&lt; send your questions in! #DellWebcast\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: Special THANKS to EVERYONE for coming out to Taboo Tuesday With DST tonight! It was FUN&amp;educational!!! :) @XiEtaDST\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: positive\n",
      "SVM+Lexicon Prediction: positive\n",
      "Lexicon Model Prediction: negative\n",
      "\n",
      "Tweet: @fatimasule That was the revelation I mentioned on sunday evening. I am still in Abj. How are u &amp; where have u been again?\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: positive\n",
      "\n",
      "Tweet: Kim Hyung Jun - Football Team the 2nd A Match at YeongDeungPo-gu DaeRimDong [12.10.27] Credit : tlxhah #6 http://t.co/u7mPTl0X\n",
      "Ground-Truth Class: neutral\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n",
      "Tweet: The audio booth is ready to blow the roof off the Comcast Center tomorrow! Are you? #MDMadness http://t.co/B19fECgY\n",
      "Ground-Truth Class: positive\n",
      "SVM Prediction: neutral\n",
      "SVM+Lexicon Prediction: neutral\n",
      "Lexicon Model Prediction: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_tweets = 0\n",
    "for text, svm_pred, svm_lex_pred, lex_pred, y  in zip(X_txt_test, svm_test_predictions, svm_lex_test_predictions, lex_test_preds, y_test):\n",
    "    print(\"Tweet: {}\".format(text))\n",
    "    print(\"Ground-Truth Class: {}\".format(y))\n",
    "    print(\"SVM Prediction: {}\".format(svm_pred))\n",
    "    print(\"SVM+Lexicon Prediction: {}\".format(svm_lex_pred))\n",
    "    print(\"Lexicon Model Prediction: {}\".format(lex_pred))\n",
    "    print()\n",
    "    \n",
    "    num_tweets += 1\n",
    "    if num_tweets == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following tasks:\n",
    " \n",
    "- Manually annotate all of the tweets printed above:\n",
    "   1. Tweet 1 Annotation: Neutral \n",
    "   1. Tweet 2 Annotation: Neutral \n",
    "   1. Tweet 3 Annotation: Positive \n",
    "   1. Tweet 4 Annotation: Neutral \n",
    "   1. Tweet 5 Annotation: Negative \n",
    "   1. Tweet 6 Annotation: Positive \n",
    "   1. Tweet 7 Annotation: Negative \n",
    "   1. Tweet 8 Annotation: Positive \n",
    "   1. Tweet 9 Annotation: Positive \n",
    "   1. Tweet 10 Annotation: Positive \n",
    "   1. Tweet 11 Annotation: Negative \n",
    "   1. Tweet 12 Annotation: Negative \n",
    "   1. Tweet 13 Annotation: Positive \n",
    "   1. Tweet 14 Annotation: Negative \n",
    "   1. Tweet 15 Annotation: Positive \n",
    "   1. Tweet 16 Annotation: Positive \n",
    "   1. Tweet 17 Annotation: Positive \n",
    "   1. Tweet 18 Annotation: Neutral \n",
    "   1. Tweet 19 Annotation: Neutral \n",
    "   1. Tweet 20 Annotation: Positive \n",
    "\n",
    "- How many of your annotations match the ground truth labels? Do you think the datasets labels are correct? (Use your intuition)\n",
    "    - 14/20. For the most part, I would say they are correct. While annotators will have differing opinions, they are close enough to deem correct for supervised machine learning. \n",
    "\n",
    "- How many of your annotations match the lexicon-based model's predictions?\n",
    "    - 7/20. \n",
    "\n",
    "- How many of your annotations match the SVM's predictions?\n",
    "    - 10/20. Half of them matched. \n",
    "    \n",
    "- How many of your annotations match the SVM+Lexicon's predictions?\n",
    "    - 10/20. Half of them matched. \n",
    "    \n",
    "- Do you see any major limitations of the linear SVM model? Please describe and provide examples below:\n",
    "\n",
    "The major limitation of the linear SVM is that it struggles to recognize negative sentiment. The model only classified a tweet as negative once, and it was due to the word \"no\". Additionally, it seems to choose neutral  majority of the time. This is like the issue of accuracy and precision. The model has a decent accuracy, but is not very useful when it comes to precision in making a positive or negative classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (1 point)\n",
    "\n",
    "For this exercise, you should come up with 10 other potential features that could be useful for sentiment analysis. You do not need to implement them. You simply need to list this. Make sure it is easy for me to understand the feature you describe. An example could be \"The count of the number of capitalized words in the text\".\n",
    "\n",
    "1. Count of exclamation points\n",
    "2. Boolean for double negatives (ex. \"We can't not celebrate!\" 2 negatives = positive)\n",
    "3. Determine emoji sentiment (ex. smiley faces = positive, angry faces or face palm = negative)\n",
    "4. Identify contradicting bigrams (ex. not great = negative)\n",
    "5. Weights for capitalized words to make them more important\n",
    "6. Weights for additional \"supportive\" words (ex. really good = +2 positive, very bad = +2 negative) \n",
    "7. Recognize symbols used as emojis (ex. \":)\" = positive, \":(\" = negative) \n",
    "8. Count of consecutive punctuation to recognize elipses or exaggeration\n",
    "9. Truncating words to make sure all versions are included. (ex. sad, sadly, saddening would truncate to sad)\n",
    "10. Weights to add order to positive and negative words, so \"great\" would be worth more than \"good\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit 1 (2 points)\n",
    "\n",
    "For this extra credit the only goal is to improve your model on the test set (i.e., increase the **macro** f1 score). You may create new features, grid search over more parameters, try different feature weighting methods (e.g., TfidfVectorizer), or test different machine learning models. You can do whatever you want as long as the final test score improves, I will provide you with the extra credit points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.6044\n",
      "Precision: 0.6846\n",
      "Recall: 0.6199\n",
      "F1: 0.6373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "X_train = vec.fit_transform(X_txt_train)\n",
    "X_test = vec.transform(X_txt_test)\n",
    "\n",
    "X_train = hstack([X_train, X_train_lexicon_features])\n",
    "X_test = hstack([X_test, X_test_lexicon_features])\n",
    "\n",
    "svc = LinearSVC()\n",
    "\n",
    "params = {'C':[0.0001, 0.001, 0.01, 0.1, 1., 10., 100.]}\n",
    "\n",
    "clf = GridSearchCV(svc, params, scoring = \"f1_macro\", cv = 10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "validation_score = clf.best_score_\n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "pred = clf.predict(X_test) \n",
    "\n",
    "precision = precision_score(y_test, pred, average = \"macro\")\n",
    "recall = recall_score(y_test, pred, average = \"macro\")\n",
    "f1 = f1_score(y_test, pred, average = \"macro\")\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1: 0.6044\n",
      "Precision: 0.6843\n",
      "Recall: 0.6193\n",
      "F1: 0.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raega\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range=(1,3))\n",
    "\n",
    "X_train = vec.fit_transform(X_txt_train)\n",
    "X_test = vec.transform(X_txt_test)\n",
    "\n",
    "X_train = hstack([X_train, X_train_lexicon_features])\n",
    "X_test = hstack([X_test, X_test_lexicon_features])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "params = {'C':[0.0001, 0.001, 0.01, 0.1, 1., 10., 100.]}\n",
    "\n",
    "clf = GridSearchCV(svc, params, scoring = \"f1_macro\", cv = 10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "validation_score = clf.best_score_\n",
    "print(\"Validation F1: {:.4f}\".format(validation_score))\n",
    "\n",
    "pred = clf.predict(X_test) \n",
    "\n",
    "precision = precision_score(y_test, pred, average = \"macro\")\n",
    "recall = recall_score(y_test, pred, average = \"macro\")\n",
    "f1 = f1_score(y_test, pred, average = \"macro\")\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
